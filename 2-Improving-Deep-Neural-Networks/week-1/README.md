# Improving Deep Neural Networks (week 1)
![Build Status](https://img.shields.io/badge/build-Stable-green.svg)
![License](https://img.shields.io/badge/license-DO_WHATEVER_YOU_WANT-green.svg)

Assignments and projects for week one of [Improving Deep Neural Networks](https://www.coursera.org/learn/deep-neural-network).

## Learning Objectives
* Recall that different types of initializations lead to different results.
* Recognize the importance of initialization in complex neural networks.
* Recognize the difference between train/dev/test sets.
* Diagnose the bias and variance issues in your model.
* Learn when and how to use regularization methods such as dropout or L2 regularization.
* Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them.
* Use gradient checking to verify the correctness of your backpropagation implementation.
<br/><br/>

## Contents
This week's content includes the following programming assignments:
* [Initialization](https://github.com/chivingtoninc/Coursera-Deep-Learning/blob/master/2-Improving-Deep-Neural-Networks/week-1/Initialization.ipynb)
* [Regularization](https://github.com/chivingtoninc/Coursera-Deep-Learning/blob/master/2-Improving-Deep-Neural-Networks/week-1/Regularization%2B-%2Bv2.ipynb)
* [Gradient Checking](https://github.com/chivingtoninc/Coursera-Deep-Learning/blob/master/2-Improving-Deep-Neural-Networks/week-1/Gradient%2BChecking%2Bv1.ipynb)
