# Improving Deep Neural Networks (week 2)
![Build Status](https://img.shields.io/badge/build-Stable-green.svg)
![License](https://img.shields.io/badge/license-DO_WHATEVER_YOU_WANT-green.svg)

Assignments and projects for week two of [Improving Deep Neural Networks](https://www.coursera.org/learn/deep-neural-network).

## Learning Objectives
* Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam.
* Use random minibatches to accelerate the convergence and improve the optimization.
* Know the benefits of learning rate decay and apply it to your optimization.
<br/><br/>

## Contents
This week's content includes the following programming assignments:
* [Optimization](https://github.com/chivingtoninc/Coursera-Deep-Learning/blob/master/2-Improving-Deep-Neural-Networks/week-2/Optimization%2Bmethods.ipynb)
